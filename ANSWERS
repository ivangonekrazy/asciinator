1. How would you measure the performance of your service?

I would Add more timing information in the logs for both the web-service
operations (receiving the file and saving it) and the ASCII conversion
operation.

I could also add metrics tracking with tools like StatsD. The ASCII conversion
operation could also be run in Python profiling tools like cProfile to gather
more granular information like memory usage and call times.

While I would also say it's important to track the logs and metrics from
live usage rather than to rely on benchmarks, tools like JMeter and Tsung
can be used to get an idea about the overall performance of the service.



2. What are some strategies you would employ to make your service more scalable?

The first step would be to run this service in an application server like
uWSGI or Apache's mod_wsgi and spawn multiple children. This would allow at
least more concurrent connections to the service.

The second step would be to instantiate multiple hosts each running an
application service (each running multiple children). These hosts would
be placed behind a load-balancer. I would do this once we see too much memory,
CPU, or disk IO pressure on a single host.

I would also think about decoupling the file upload and conversion steps further.
The web request could return immediately after the file upload and include
a link to the ASCII art image once conversion is completed.

The file upload receivers act as a producer, pushing conversion jobs into a
queue for conversion workers to consume.



3. How would your design change if you needed to store the uploaded images?

I feel it's hard to say more about this without knowing how we intend to use
the stored image files afterwards.

One change I'd make is to push the source image to S3 before conversion,
instead of saving a local tempfile. In this case, S3 would also act as
part of the layer that persists the conversion job.



4. What are the cost factors of your scaling choices? Which parts of your
solution would grow in cost the fastest?

In the current implementation, I'd also guess that the hosts responsible for
performing the conversion will likely be the initial processing and memory
bottleneck. Adding more hosts for this role will likely be the bigger cost driver.



5. Where are your critical points of failure and how would you mitigate them?

In the current implementation there is a critical hand off between the
web service that receives the uploaded file and the ASCII conversion process.
The tempfile could get unexpectedly deleted and cause a failure. The local disk
storage is a point of failure.

We could mitigate this by explicitly decoupling file reception and image
conversion. Multiple file upload hosts could push image files to S3 and have
them represented as a conversion job somewhere. Workers would pick up these
conversion jobs and notify the user upon completion.

This strategy assumes S3 and network access is more reliable than local disk
storage.



6. How given a change to the algorithm what issues do you foresee when
upgrading your scaled-out solution?

Assuming we have developed the producer-consumer implementation sketched out
in #2 and #5, we may run into the following issues:

- Changes in worklaod: The new algorithm may change compute, network, or storage
  requirements. This may require changes to how resources are allocated
  for the solution.

- Incompatible jobs: A change in algorithm may require a change in how the
  jobs are represented in the queue, which may require changes in the workers.

  We can mitigate this by versioning jobs or by temporarily placing the old
  and new job versions in two separate queues. The deprecated version of jobs
  will be consumed by the deprecated workers. Upgraded producers will only place
  jobs in the new queue, from which the upgraded workers will consume.



7. If you wanted to migrate your scaled-out solution to another cloud provider
(with comparable offerings but different APIâ€™s) how would you envision this
happening? How would deal with data consistency during the transition and
rollbacks in the event of failures?

We could do the following to mitigate the risk of failures during the migration:

A small percentage of requests are duplicated between the old and provider.
Producers and consumers in both providers will process their jobs. Results
from the new provider environment are validated against the results from the
same job in the old provider. Results from the new provider are discarded.
Debug any deltas between results from the old and new providers. Do this until
there are no differences between the results from each provider.

As we gain more confidence in the new environment we can increase the percentage
of requests copied to the new provider, carefully watching performance metrics.

Once correctness and performance are verified, we can slowly increase the
percentage of requests that are routed to the new provider instead of the old
provider. At this stage a request goes only to one provider instead of both
and the results in the new provider are not discarded. We continue this
until all requests are routed to the new provider.

At any of the above stages, we rollback by dialing back the percentage of
requests going to the new provider.


